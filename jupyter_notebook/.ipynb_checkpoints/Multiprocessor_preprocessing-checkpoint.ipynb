{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def text_preprocessing(\n",
    "    text:list,\n",
    "    punctuations = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_“~''',\n",
    "    stop_words=['and', 'a', 'is', 'the', 'in', 'be', 'will']\n",
    "    )->list:\n",
    "    \"\"\"\n",
    "    A method to preproces text\n",
    "    \"\"\"\n",
    "    for x in text.lower(): \n",
    "        if x in punctuations: \n",
    "            text = text.replace(x, \"\")\n",
    "\n",
    "    # Removing words that have numbers in them\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "\n",
    "    # Removing digits\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Setting every word to lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Converting all our text to a list \n",
    "    text = text.split(' ')\n",
    "\n",
    "    # Droping empty strings\n",
    "    text = [x for x in text if x!='']\n",
    "\n",
    "    # Droping stop words\n",
    "#     text = [x for x in text if x not in stop_words]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def process(line):\n",
    "    line = line.lower()\n",
    "    return line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type before reconstruction :  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# ran this on the workstation and copied it into the notebook\n",
    "\n",
    "import ast\n",
    "  \n",
    "# reading the data from the file\n",
    "with open('../Data_shared/wiki_subset.txt') as f:\n",
    "    data = f.read()\n",
    "      \n",
    "# reconstructing the data as a dictionary\n",
    "wikidata = ast.literal_eval(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wikidata to the subset of wikipedia dataset\n",
    "# wikidata = datasets.load_dataset('wikipedia', '20200501.en')\n",
    "# # make a subset\n",
    "# wikidata = wikidata['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{',\n",
       " \"'\",\n",
       " 't',\n",
       " 'i',\n",
       " 't',\n",
       " 'l',\n",
       " 'e',\n",
       " \"'\",\n",
       " ':',\n",
       " ' ',\n",
       " '[',\n",
       " \"'\",\n",
       " 'y',\n",
       " 'a',\n",
       " 'n',\n",
       " 'g',\n",
       " 'l',\n",
       " 'i',\n",
       " 'u',\n",
       " 'q',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " \"'\",\n",
       " ',',\n",
       " ' ',\n",
       " \"'\",\n",
       " 'o',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " ' ',\n",
       " 'a',\n",
       " 'u',\n",
       " 's',\n",
       " 't',\n",
       " 'r',\n",
       " 'a',\n",
       " 'l',\n",
       " 'i',\n",
       " 'a',\n",
       " ' ',\n",
       " 'l',\n",
       " 't',\n",
       " 'd',\n",
       " \"'\",\n",
       " ',',\n",
       " ' ',\n",
       " '\"',\n",
       " 's',\n",
       " 't',\n",
       " '.',\n",
       " ' ',\n",
       " 'm',\n",
       " 'a',\n",
       " 'r',\n",
       " 'y',\n",
       " \"'\",\n",
       " 's',\n",
       " ' ',\n",
       " 'c',\n",
       " 'h',\n",
       " 'u',\n",
       " 'r',\n",
       " 'c',\n",
       " 'h',\n",
       " ',',\n",
       " ' ',\n",
       " 's',\n",
       " 'ø',\n",
       " 'n',\n",
       " 'd',\n",
       " 'e',\n",
       " 'r',\n",
       " 'b',\n",
       " 'o',\n",
       " 'r',\n",
       " 'g',\n",
       " '\"',\n",
       " ',',\n",
       " ' ',\n",
       " \"'\",\n",
       " 'k',\n",
       " 'a',\n",
       " 'l',\n",
       " 'i',\n",
       " 't',\n",
       " 't',\n",
       " 'a',\n",
       " \"'\",\n",
       " ',',\n",
       " ' ',\n",
       " \"'\",\n",
       " 'w',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-915e4435ef77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#create jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunkStart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunkSize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunkify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Data_shared/wiki_subset.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_wrapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunkStart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunkSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-915e4435ef77>\u001b[0m in \u001b[0;36mchunkify\u001b[0;34m(fname, size)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mchunkStart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunkEnd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mchunkEnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_baroni(neg_file, pos_file):\n",
    "    filenames = [\"neg_file\", \"pos_file\"]\n",
    "\n",
    "    for i, file in enumerate([neg_file, pos_file]):\n",
    "        globals()['results_{}'.format(filenames[i])] = []\n",
    "        \n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                globals()['results_{}'.format(filenames[i])].append(line.replace(\"-n\", \"\").replace(\"\\n\", \"\").strip(\"\").split(\"\\t\"))\n",
    "                line = f.readline()\n",
    "        f.close()\n",
    "\n",
    "    baroni = sum(results_neg_file, []) + sum(results_pos_file, [])\n",
    "    baroni_set = set(baroni)\n",
    "\n",
    "    return results_neg_file, results_pos_file, baroni, baroni_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_file = \"../Data_Shared/eacl2012-data/negative-examples.txtinput\"\n",
    "pos_file = \"../Data_Shared/eacl2012-data/positive-examples.txtinput\"\n",
    "results_neg_file, results_pos_file, baroni, baroni_set = import_baroni(neg_file, pos_file)\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "batch_size = 128\n",
    "unk_thresh = 1\n",
    "\n",
    "seqs = baroni"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
