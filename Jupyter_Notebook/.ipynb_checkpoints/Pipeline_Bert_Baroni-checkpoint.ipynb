{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import pickle5 as pickle \n",
    "from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from transformers import (DistilBertTokenizerFast, DistilBertModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vocab:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._tok_counts = Counter()\n",
    "        self._id_to_tok = {}\n",
    "\n",
    "    def fit(self, data, word_list):\n",
    "        for sequence in data:\n",
    "            self._tok_counts.update([tok for tok in sequence if tok in word_list])\n",
    "\n",
    "        self._toks = ([\"</s>\", \"<unk>\"] +\n",
    "                      [tok for tok, _ in self._tok_counts.most_common()])\n",
    "        self._tok_to_id = {tok: i for i, tok in enumerate(self._toks)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._toks)\n",
    "    \n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._t = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "    def words(self, sequences: List[str]):\n",
    "        return [s.split() for s in sequences]\n",
    "\n",
    "    def __call__(self, sequences: List[str]):\n",
    "        words = self.words(sequences)\n",
    "        subw = self._t.batch_encode_plus(words,\n",
    "                                         is_split_into_words=True,\n",
    "                                         padding=True)\n",
    "        return words, subw\n",
    "\n",
    "class EmbedAverages(torch.nn.Module):\n",
    "    def __init__(self, n_words, dim):\n",
    "        super().__init__()\n",
    "        # matrix of wordvector sums\n",
    "        self.register_buffer(\"_sum\", torch.zeros(n_words, dim))\n",
    "        self.register_buffer(\"_counts\", torch.zeros(n_words, dtype=torch.long))\n",
    "        self.register_buffer(\"_cov\", torch.zeros(n_words, dim, dim))\n",
    "    \n",
    "    def add(self, ix, vec):\n",
    "        self._counts[ix] += 1\n",
    "        self._sum[ix] += vec\n",
    "        self._cov[ix] += vec.reshape([len(vec), 1]) @ vec.reshape([1, len(vec)])\n",
    "    \n",
    "    def get_mean_covariance(self, ix):\n",
    "#         print(\"self._counts[ix]\", self._counts[ix])\n",
    "#         print(\"self._sum[ix]\", self._sum[ix])\n",
    "        \n",
    "        mean = self._sum[ix] / self._counts[ix]\n",
    "        d = len(mean)\n",
    "        cov = self._cov[ix] / self._counts[ix] - mean.reshape([d, 1])  @ mean.reshape([1, d])\n",
    "        cov = .001 * torch.eye(d) + cov\n",
    "        return mean, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_kl(wordpair):\n",
    "    # Get the mean vectors and covariance matrices for the two words in the word pair\n",
    "    mean1, covariance_matrix1 = embavg.get_mean_covariance(vocab._tok_to_id.get(wordpair[0])) \n",
    "    mean2, covariance_matrix2 = embavg.get_mean_covariance(vocab._tok_to_id.get(wordpair[1])) \n",
    "    \n",
    "    # Create PyTorch multivariate normal distributions using the mean vectors and covariance matrices\n",
    "    p = torch.distributions.multivariate_normal.MultivariateNormal(mean1, covariance_matrix=covariance_matrix1)\n",
    "    q = torch.distributions.multivariate_normal.MultivariateNormal(mean2, covariance_matrix=covariance_matrix2)\n",
    "\n",
    "    # Calculate the KL divergence between the two distributions\n",
    "    kl = torch.distributions.kl.kl_divergence(p, q)\n",
    "\n",
    "    return kl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    nominator = torch.dot(a, b)\n",
    "    \n",
    "    a_norm = torch.sqrt(torch.sum(a**2))\n",
    "    b_norm = torch.sqrt(torch.sum(b**2))\n",
    "    \n",
    "    denominator = a_norm * b_norm\n",
    "    \n",
    "    cosine_similarity = nominator / denominator\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_baroni(neg_file, pos_file):\n",
    "    filenames = [\"neg_file\", \"pos_file\"]\n",
    "\n",
    "    for i, file in enumerate([neg_file, pos_file]):\n",
    "        globals()['results_{}'.format(filenames[i])] = []\n",
    "        \n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                globals()['results_{}'.format(filenames[i])].append(line.replace(\"-n\", \"\").replace(\"\\n\", \"\").strip(\"\").split(\"\\t\"))\n",
    "                line = f.readline()\n",
    "        f.close()\n",
    "\n",
    "    baroni = sum(results_neg_file, []) + sum(results_pos_file, [])\n",
    "    baroni_set = set(baroni)\n",
    "\n",
    "    return results_neg_file, results_pos_file, baroni, baroni_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5163\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode\n",
    "with open(\"../data_distrembed/roen.vocab\", \"r\") as f:\n",
    "    # Read the contents of the file\n",
    "    contents = f.read()\n",
    "\n",
    "print(len(contents))  # prints the contents of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_file = \"../Data_Shared/eacl2012-data/negative-examples.txtinput\"\n",
    "pos_file = \"../Data_Shared/eacl2012-data/positive-examples.txtinput\"\n",
    "results_neg_file, results_pos_file, baroni, baroni_set = import_baroni(neg_file, pos_file)\n",
    "\n",
    "with open('../Data_Shared/wiki_subtext_preprocess.pickle', 'rb') as handle:\n",
    "        seqs = pickle.load(handle)\n",
    "\n",
    "import ast\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "\n",
    "  \n",
    "# reading the data from the file\n",
    "with open('../Data_shared/wiki_subset.txt') as f:\n",
    "    data = f.read()\n",
    "      \n",
    "# reconstructing the data as a dictionary\n",
    "wikidata = ast.literal_eval(data)\n",
    "\n",
    "seqs = [sentence.strip() for seq in wikidata for sentence in seq.split(\".\")]\n",
    "tok = Tokenizer()\n",
    "vocab = Vocab()\n",
    "vocab.fit(tok.words(seqs), baroni)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 14664.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "with open('../Data_shared/wiki_subset.txt') as f:\n",
    "    data = f.read()\n",
    "\n",
    "wikidata = ast.literal_eval(data)\n",
    "\n",
    "wikidata = wikidata[\"text\"][:5000]   \n",
    "\n",
    "max_length = 200\n",
    "\n",
    "wikidata = [sentence[:max_length].strip() if len(sentence.split()) > max_length else sentence.strip()\n",
    "        for seq in tqdm(wikidata)\n",
    "        for sentence in seq.split(\".\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "for i in wikidata:\n",
    "    if len(i.split()) > 198:\n",
    "        print(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "embavg = torch.load('../data_distrembed/first100000.avgs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab._tok_counts))\n",
    "# print(vocab._tok_counts)\n",
    "for key, item in vocab._tok_counts.items():\n",
    "    if key not in baroni:\n",
    "        print(key, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "baroni_pos_subset = [x for x in results_pos_file if x[0] in vocab._tok_counts and x[1] in vocab._tok_counts]\n",
    "baroni_neg_subset = [x for x in results_neg_file if x[0] in vocab._tok_counts and x[1] in vocab._tok_counts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Wordpair  True label\n",
      "0        [acid, chemical]           1\n",
      "1    [affection, feeling]           1\n",
      "2     [aircraft, vehicle]           1\n",
      "3         [alpha, symbol]           1\n",
      "4        [antiquity, era]           1\n",
      "..                    ...         ...\n",
      "732     [woman, mistress]           0\n",
      "733         [wood, maple]           0\n",
      "734          [work, bird]           0\n",
      "735   [writer, dramatist]           0\n",
      "736        [writer, poet]           0\n",
      "\n",
      "[737 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# baroni_pos_subset, baroni_neg_subset = create_combined_subset(word_cov_matrices, results_neg_file, results_pos_file, combined_set)\n",
    "\n",
    "baroni_subset_label = []\n",
    "\n",
    "for i in baroni_pos_subset:\n",
    "    baroni_subset_label.append([i, 1])\n",
    "\n",
    "for i in baroni_neg_subset:\n",
    "    baroni_subset_label.append([i, 0])\n",
    "\n",
    "# MAKE DATAFRAME\n",
    "df1 = pd.DataFrame(baroni_subset_label, columns =['Wordpair', 'True label'])\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "baroni_subset_cos = []\n",
    "\n",
    "for wordpair in (baroni_pos_subset + baroni_neg_subset):\n",
    "    A = embavg._sum[vocab._tok_to_id.get(wordpair[0])]\n",
    "    B = embavg._sum[vocab._tok_to_id.get(wordpair[1])]\n",
    "    baroni_subset_cos.append(torch.cosine_similarity(A, B))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate KL and COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737/737 [01:02<00:00, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Wordpair  True label      KL score       COS score\n",
      "0        [acid, chemical]           1  22588.404297  tensor(0.7882)\n",
      "1    [affection, feeling]           1  24089.642578  tensor(0.7085)\n",
      "2     [aircraft, vehicle]           1  22337.914062  tensor(0.7861)\n",
      "3         [alpha, symbol]           1  47081.132812  tensor(0.6093)\n",
      "4        [antiquity, era]           1  34338.492188  tensor(0.6267)\n",
      "..                    ...         ...           ...             ...\n",
      "732     [woman, mistress]           0  33531.156250  tensor(0.6877)\n",
      "733         [wood, maple]           0  25387.968750  tensor(0.7393)\n",
      "734          [work, bird]           0  37608.054688  tensor(0.6506)\n",
      "735   [writer, dramatist]           0  50024.500000  tensor(0.6341)\n",
      "736        [writer, poet]           0  19814.851562  tensor(0.8291)\n",
      "\n",
      "[737 rows x 4 columns]\n",
      "COS AP:  0.6932439050975777\n",
      "KL AP:  0.6869634675826695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CALCULATE KL and COS\n",
    "baroni_subset_kl = []\n",
    "baroni_subset_cos = []\n",
    "\n",
    "for wordpair in tqdm((baroni_pos_subset + baroni_neg_subset)):\n",
    "    baroni_subset_kl.append(calculate_kl(wordpair))\n",
    "    baroni_subset_cos.append(cosine_similarity(embavg._sum[vocab._tok_to_id.get(wordpair[0])], \n",
    "                                               embavg._sum[vocab._tok_to_id.get(wordpair[1])]))\n",
    "\n",
    "df1['KL score'] = baroni_subset_kl\n",
    "df1['COS score'] = baroni_subset_cos\n",
    "\n",
    "# with open('df1.pickle', 'wb') as handle:\n",
    "#     pickle.dump(df1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(df1)\n",
    "print(\"COS AP: \", average_precision_score(df1[\"True label\"], df1[\"COS score\"]))\n",
    "print(\"KL AP: \", average_precision_score(df1[\"True label\"], -df1[\"KL score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
