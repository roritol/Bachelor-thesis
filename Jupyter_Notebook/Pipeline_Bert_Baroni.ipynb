{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import pickle5 as pickle \n",
    "from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from transformers import (DistilBertTokenizerFast, DistilBertModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vocab:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._tok_counts = Counter()\n",
    "        self._id_to_tok = {}\n",
    "\n",
    "    def fit(self, data, word_list):\n",
    "        for sequence in data:\n",
    "            self._tok_counts.update([tok for tok in sequence if tok in word_list])\n",
    "\n",
    "        self._toks = ([\"</s>\", \"<unk>\"] +\n",
    "                      [tok for tok, _ in self._tok_counts.most_common()])\n",
    "        self._tok_to_id = {tok: i for i, tok in enumerate(self._toks)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._toks)\n",
    "    \n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._t = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "    def words(self, sequences: List[str]):\n",
    "        return [s.split() for s in sequences]\n",
    "\n",
    "    def __call__(self, sequences: List[str]):\n",
    "        words = self.words(sequences)\n",
    "        subw = self._t.batch_encode_plus(words,\n",
    "                                         is_split_into_words=True,\n",
    "                                         padding=True)\n",
    "        return words, subw\n",
    "\n",
    "class EmbedAverages(torch.nn.Module):\n",
    "    def __init__(self, n_words, dim):\n",
    "        super().__init__()\n",
    "        # matrix of wordvector sums\n",
    "        self.register_buffer(\"_sum\", torch.zeros(n_words, dim))\n",
    "        self.register_buffer(\"_counts\", torch.zeros(n_words, dtype=torch.long))\n",
    "        self.register_buffer(\"_cov\", torch.zeros(n_words, dim, dim))\n",
    "    \n",
    "    def add(self, ix, vec):\n",
    "        self._counts[ix] += 1\n",
    "        self._sum[ix] += vec\n",
    "        self._cov[ix] += vec.reshape([len(vec), 1]) @ vec.reshape([1, len(vec)])\n",
    "    \n",
    "    def get_mean_covariance(self, ix):\n",
    "#         print(\"self._counts[ix]\", self._counts[ix])\n",
    "#         print(\"self._sum[ix]\", self._sum[ix])\n",
    "        \n",
    "        mean = self._sum[ix] / self._counts[ix]\n",
    "        d = len(mean)\n",
    "        cov = self._cov[ix] / self._counts[ix] - mean.reshape([d, 1])  @ mean.reshape([1, d])\n",
    "        cov = .001 * torch.eye(d) + cov\n",
    "        return mean, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_kl(wordpair):\n",
    "    # Get the mean vectors and covariance matrices for the two words in the word pair\n",
    "    mean1, covariance_matrix1 = embavg.get_mean_covariance(vocab._tok_to_id.get(wordpair[0])) \n",
    "    mean2, covariance_matrix2 = embavg.get_mean_covariance(vocab._tok_to_id.get(wordpair[1])) \n",
    "    \n",
    "    # Create PyTorch multivariate normal distributions using the mean vectors and covariance matrices\n",
    "    p = torch.distributions.multivariate_normal.MultivariateNormal(mean1, covariance_matrix=covariance_matrix1)\n",
    "    q = torch.distributions.multivariate_normal.MultivariateNormal(mean2, covariance_matrix=covariance_matrix2)\n",
    "\n",
    "    # Calculate the KL divergence between the two distributions\n",
    "    kl = torch.distributions.kl.kl_divergence(p, q)\n",
    "\n",
    "    return kl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    nominator = torch.dot(a, b)\n",
    "    \n",
    "    a_norm = torch.sqrt(torch.sum(a**2))\n",
    "    b_norm = torch.sqrt(torch.sum(b**2))\n",
    "    \n",
    "    denominator = a_norm * b_norm\n",
    "    \n",
    "    cosine_similarity = nominator / denominator\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_baroni(neg_file, pos_file):\n",
    "    filenames = [\"neg_file\", \"pos_file\"]\n",
    "\n",
    "    for i, file in enumerate([neg_file, pos_file]):\n",
    "        globals()['results_{}'.format(filenames[i])] = []\n",
    "        \n",
    "        with open(file) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                globals()['results_{}'.format(filenames[i])].append(line.replace(\"-n\", \"\").replace(\"\\n\", \"\").strip(\"\").split(\"\\t\"))\n",
    "                line = f.readline()\n",
    "        f.close()\n",
    "\n",
    "    baroni = sum(results_neg_file, []) + sum(results_pos_file, [])\n",
    "    baroni_set = set(baroni)\n",
    "\n",
    "    return results_neg_file, results_pos_file, baroni, baroni_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5163\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode\n",
    "with open(\"../data_distrembed/roen.vocab\", \"r\") as f:\n",
    "    # Read the contents of the file\n",
    "    contents = f.read()\n",
    "\n",
    "print(len(contents))  # prints the contents of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neg_file = \"../Data_Shared/eacl2012-data/negative-examples.txtinput\"\n",
    "pos_file = \"../Data_Shared/eacl2012-data/positive-examples.txtinput\"\n",
    "results_neg_file, results_pos_file, baroni, baroni_set = import_baroni(neg_file, pos_file)\n",
    "\n",
    "with open('../Data_Shared/wiki_subtext_preprocess.pickle', 'rb') as handle:\n",
    "        seqs = pickle.load(handle)\n",
    "\n",
    "import ast\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "\n",
    "  \n",
    "# reading the data from the file\n",
    "with open('../Data_shared/wiki_subset.txt') as f:\n",
    "    data = f.read()\n",
    "      \n",
    "# reconstructing the data as a dictionary\n",
    "wikidata = ast.literal_eval(data)\n",
    "\n",
    "wikidata = wikidata[\"text\"][:100]\n",
    "seqs = [sentence.strip() for seq in seqs for sentence in seq.split(\".\")]\n",
    "print(seqs)\n",
    "tok = Tokenizer()\n",
    "vocab = Vocab()\n",
    "vocab.fit(tok.words(seqs), baroni)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Yangliuqing () is a market town in Xiqing District, in the western suburbs of Tianjin, People's Republic of China.\", 'Despite its relatively small size, it has been named since 2006 in the \"famous historical and cultural market towns in China\".', 'It is best known in China for creating nianhua or Yangliuqing nianhua.', 'For more than 400 years, Yangliuqing has in effect specialised in the creation of these woodcuts for the New Year.', \"wood block prints using vivid colourschemes to portray traditional scenes of children's games often interwoven with auspiciouse objects.\", ', it had 27 residential communities () and 25 villages under its administration.', 'Shi Family Grand Courtyard\\n\\nShi Family Grand Courtyard (Tiānjīn Shí Jiā Dà Yuàn, 天津石家大院) is situated in Yangliuqing Town of Xiqing District, which is the former residence of wealthy merchant Shi Yuanshi - the 4th son of Shi Wancheng, one of the eight great masters in Tianjin.', 'First built in 1875, it covers over 6,000 square meters, including large and small yards and over 200 folk houses, a theater and over 275 rooms that served as apartments and places of business and worship for this powerful family.', 'Shifu Garden, which finished its expansion in October 2003, covers 1,200 square meters, incorporates the elegance of imperial garden and delicacy of south garden.', 'Now the courtyard of Shi family covers about 10,000 square meters, which is called the first mansion in North China.', 'Now it serves as the folk custom museum in Yangliuqing, which has a large collection of folk custom museum in Yanliuqing, which has a large collection of folk art pieces like Yanliuqing New Year pictures, brick sculpture.', \"Shi's ancestor came from Dong'e County in Shandong Province, engaged in water transport of grain.\", 'As the wealth gradually accumulated, the Shi Family moved to Yangliuqing and bought large tracts of land and set up their residence.', 'Shi Yuanshi came from the fourth generation of the family, who was a successful businessman and a good household manager, and the residence was thus enlarged for several times until it acquired the present scale.', 'It is believed to be the first mansion in the west of Tianjin.', 'The residence is symmetric based on the axis formed by a passageway in the middle, on which there are four archways.', 'On the east side of the courtyard, there are traditional single-story houses with rows of rooms around the four sides, which was once the living area for the Shi Family.', \"The rooms on north side were the accountants' office.\", 'On the west are the major constructions including the family hall for worshipping Buddha, theater and the south reception room.', 'On both sides of the residence are side yard rooms for maids and servants.', \"Today, the Shi mansion, located in the township of Yangliuqing to the west of central Tianjin, stands as a surprisingly well-preserved monument to China's pre-revolution mercantile spirit.\", \"It also serves as an on-location shoot for many of China's popular historical dramas.\", 'Many of the rooms feature period furniture, paintings and calligraphy, and the extensive Shifu Garden.', \"Part of the complex has been turned into the Yangliuqing Museum, which includes displays focused on symbolic aspects of the courtyards'  construction, local folk art and customs, and traditional period furnishings and crafts.\", 'See also \\n\\nList of township-level divisions of Tianjin\\n\\nReferences \\n\\n http://arts.cultural-china.com/en/65Arts4795.html\\n\\nCategory:Towns in Tianjin'], ['Orana Australia Ltd is a not-for-profit organisation that provides a diverse range of training and support services to over 650 people with disabilities and their families in South Australia.', 'History\\nThe Mentally Retarded Children’s Society of SA Inc. was established in 1950 by a group of parents who wanted education, employment and accommodation opportunities for their children within the local community at a time when institutionalised care in Adelaide was their only alternative.', 'The society’s aims were to seek education or training facilities for people with intellectual disabilities, to establish sheltered workshops, and to establish residential hostels.', 'A number of sheltered workshops were established, and in 1980, the name was changed to the Aboriginal word \"Orana\", which means \"Welcome\".', 'Today, Orana provides assisted employment, assisted accommodation and respite services to people with intellectual disabilities.', \"Orana's current and previous clients include Mitsubishi Motors, Clipsal, RAA, Elders Limited, and Billycart Kids.\", 'Orana was one of the first disability service organisations to achieve Quality Accreditation.', 'The services and products they offer are:\\n\\n Packaging\\n Assembly\\n Sewing\\n Collating & Mailing\\n Furniture - Retail\\n Furniture – Manufacture for Commercial Market\\n Worm Farming\\n Work Crews\\n Pet & Grain – Retail\\n\\nIn 2018, after 65 years of bettering people’s lives, Orana identified a community need and expanded their operations into the aged care sector.', 'After the unveiling of the Australian Government’s Commonwealth Home Support Programme (CHSP) and seeing it as a natural step of progression, Orana now provides quality tailored aged care at home.', 'The well-resourced organization delivers help across a range of areas, helping the elderly remain where they want to be - in the comfort of their own home during their later years.', 'Orana continues with its mission to support people remain independent, valued and productive members of the community.', 'References\\n\\nExternal links \\n \\n\\nCategory:Disability organisations based in Australia\\nCategory:Organisations based in South Australia']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# for line in wikidata[\"text\"][:100][:1]:\n",
    "#     print(tokenizer.tokenize(line))\n",
    "\n",
    "seqs = [tokenizer.tokenize(line) for line in wikidata[\"text\"][:100][:2]]\n",
    "\n",
    "print(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "embavg = torch.load('../data_distrembed/first100000.avgs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab._tok_counts))\n",
    "# print(vocab._tok_counts)\n",
    "for key, item in vocab._tok_counts.items():\n",
    "    if key not in baroni:\n",
    "        print(key, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "baroni_pos_subset = [x for x in results_pos_file if x[0] in vocab._tok_counts and x[1] in vocab._tok_counts]\n",
    "baroni_neg_subset = [x for x in results_neg_file if x[0] in vocab._tok_counts and x[1] in vocab._tok_counts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Wordpair  True label\n",
      "0        [acid, chemical]           1\n",
      "1    [affection, feeling]           1\n",
      "2     [aircraft, vehicle]           1\n",
      "3         [alpha, symbol]           1\n",
      "4        [antiquity, era]           1\n",
      "..                    ...         ...\n",
      "732     [woman, mistress]           0\n",
      "733         [wood, maple]           0\n",
      "734          [work, bird]           0\n",
      "735   [writer, dramatist]           0\n",
      "736        [writer, poet]           0\n",
      "\n",
      "[737 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# baroni_pos_subset, baroni_neg_subset = create_combined_subset(word_cov_matrices, results_neg_file, results_pos_file, combined_set)\n",
    "\n",
    "baroni_subset_label = []\n",
    "\n",
    "for i in baroni_pos_subset:\n",
    "    baroni_subset_label.append([i, 1])\n",
    "\n",
    "for i in baroni_neg_subset:\n",
    "    baroni_subset_label.append([i, 0])\n",
    "\n",
    "# MAKE DATAFRAME\n",
    "df1 = pd.DataFrame(baroni_subset_label, columns =['Wordpair', 'True label'])\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "baroni_subset_cos = []\n",
    "\n",
    "for wordpair in (baroni_pos_subset + baroni_neg_subset):\n",
    "    A = embavg._sum[vocab._tok_to_id.get(wordpair[0])]\n",
    "    B = embavg._sum[vocab._tok_to_id.get(wordpair[1])]\n",
    "    baroni_subset_cos.append(torch.cosine_similarity(A, B))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate KL and COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737/737 [01:02<00:00, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Wordpair  True label      KL score       COS score\n",
      "0        [acid, chemical]           1  22588.404297  tensor(0.7882)\n",
      "1    [affection, feeling]           1  24089.642578  tensor(0.7085)\n",
      "2     [aircraft, vehicle]           1  22337.914062  tensor(0.7861)\n",
      "3         [alpha, symbol]           1  47081.132812  tensor(0.6093)\n",
      "4        [antiquity, era]           1  34338.492188  tensor(0.6267)\n",
      "..                    ...         ...           ...             ...\n",
      "732     [woman, mistress]           0  33531.156250  tensor(0.6877)\n",
      "733         [wood, maple]           0  25387.968750  tensor(0.7393)\n",
      "734          [work, bird]           0  37608.054688  tensor(0.6506)\n",
      "735   [writer, dramatist]           0  50024.500000  tensor(0.6341)\n",
      "736        [writer, poet]           0  19814.851562  tensor(0.8291)\n",
      "\n",
      "[737 rows x 4 columns]\n",
      "COS AP:  0.6932439050975777\n",
      "KL AP:  0.6869634675826695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CALCULATE KL and COS\n",
    "baroni_subset_kl = []\n",
    "baroni_subset_cos = []\n",
    "\n",
    "for wordpair in tqdm((baroni_pos_subset + baroni_neg_subset)):\n",
    "    baroni_subset_kl.append(calculate_kl(wordpair))\n",
    "    baroni_subset_cos.append(cosine_similarity(embavg._sum[vocab._tok_to_id.get(wordpair[0])], \n",
    "                                               embavg._sum[vocab._tok_to_id.get(wordpair[1])]))\n",
    "\n",
    "df1['KL score'] = baroni_subset_kl\n",
    "df1['COS score'] = baroni_subset_cos\n",
    "\n",
    "# with open('df1.pickle', 'wb') as handle:\n",
    "#     pickle.dump(df1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(df1)\n",
    "print(\"COS AP: \", average_precision_score(df1[\"True label\"], df1[\"COS score\"]))\n",
    "print(\"KL AP: \", average_precision_score(df1[\"True label\"], -df1[\"KL score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
